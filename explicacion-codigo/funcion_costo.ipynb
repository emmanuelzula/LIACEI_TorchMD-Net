{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186ef773-00bb-485a-89f4-186130915309",
   "metadata": {},
   "source": [
    "# Función de costo dentro de la arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5ccfc-8b3e-490f-98f3-052d28c23086",
   "metadata": {},
   "source": [
    "## Ejemplo mas simple del uso de `pytorch_lightning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540463ca-dca3-445b-af23-d7099da53030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Preparación de los datos\n",
    "X_train = torch.randn(1000, 10)\n",
    "y_train = torch.randn(1000, 1)\n",
    "X_val = torch.randn(200, 10)\n",
    "y_val = torch.randn(200, 1)\n",
    "X_test = torch.randn(200, 10)\n",
    "y_test = torch.randn(200, 1)\n",
    "\n",
    "# Definición del datamodule\n",
    "class SimpleDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TensorDataset(self.X_val, self.y_val)\n",
    "        return DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TensorDataset(self.X_test, self.y_test)\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "# Definición del modelo\n",
    "class SimpleModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x)\n",
    "        loss = torch.nn.functional.mse_loss(output, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x)\n",
    "        loss = torch.nn.functional.mse_loss(output, y)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x)\n",
    "        loss = torch.nn.functional.mse_loss(output, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "datamodule = SimpleDataModule(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "model = SimpleModel()\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5562a29-dde1-4236-992a-57e8be7a1ac1",
   "metadata": {},
   "source": [
    "## Resumen del Funcionamiento del Código\n",
    "\n",
    "El código proporcionado define una clase llamada `LNNP` que sirve como un contenedor para un modelo de redes neuronales potenciales (NNP, por sus siglas en inglés) implementado en TorchMD-Net utilizando PyTorch Lightning. A continuación se describe el funcionamiento general del código:\n",
    "\n",
    "- **Importaciones**: Se importan las bibliotecas necesarias, incluyendo PyTorch, PyTorch Lightning y módulos específicos de TorchMD-Net.\n",
    "\n",
    "- **Clase `LNNP`**: Se define la clase `LNNP`, que hereda de `LightningModule`. Esta clase encapsula la lógica del modelo NNP y proporciona métodos para entrenamiento, validación y prueba.\n",
    "\n",
    "- **Inicialización**: En el método `__init__`, se configuran los hiperparámetros, se crea o carga el modelo NNP, se inicializan variables para suavizado exponencial y almacenamiento de pérdidas.\n",
    "\n",
    "- **Configuración de Optimizadores**: En el método `configure_optimizers`, se configuran el optimizador AdamW y el planificador de tasa de aprendizaje ReduceLROnPlateau.\n",
    "\n",
    "- **Paso Forward**: En el método `forward`, se realiza el paso hacia adelante del modelo NNP para generar predicciones a partir de datos de entrada.\n",
    "\n",
    "- **Paso de Entrenamiento**: En el método `training_step`, se realiza un paso de entrenamiento utilizando la función de pérdida MSE.\n",
    "\n",
    "- **Paso de Validación y Prueba**: En los métodos `validation_step` y `test_step`, se realizan pasos de validación y prueba respectivamente, utilizando la función de pérdida L1.\n",
    "\n",
    "- **Cálculo de Pérdidas y Actualización de Parámetros**: En el método `step`, se calculan las pérdidas y se actualizan los parámetros del modelo.\n",
    "\n",
    "- **Actualización del Optimizador**: En el método `optimizer_step`, se realiza un paso de optimización, incluyendo el escalado de la tasa de aprendizaje durante el calentamiento.\n",
    "\n",
    "- **Registro de Métricas**: En los métodos `training_epoch_end` y `validation_epoch_end`, se registran métricas como pérdidas y tasas de aprendizaje al final de cada época de entrenamiento y validación.\n",
    "\n",
    "- **Reinicio de Pérdidas y EMA**: En los métodos `_reset_losses_dict` y `_reset_ema_dict`, se reinician las variables de pérdidas y suavizado exponencial móvil al comienzo de cada época.\n",
    "\n",
    "Este resumen proporciona una visión general del funcionamiento del código, destacando los principales componentes y procesos involucrados en el entrenamiento, validación y prueba del modelo NNP utilizando PyTorch Lightning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a589e-2b16-4809-ae62-b2fae9fc0cf8",
   "metadata": {},
   "source": [
    "## Implementación dentro de la arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbe8d6-e678-4cda-be5e-e13792f5a354",
   "metadata": {},
   "source": [
    "## Importaciones\r\n",
    "\r\n",
    "Importamos las bibliotecas necesarias para nuestro módulo. Esto incluye:\r\n",
    "\r\n",
    "- `torch`: La biblioteca principal de PyTorch para cálculos de tensor y operaciones en GPU.\r\n",
    "- `AdamW`: Un optimizador que implementa el algoritmo Adam con corrección de peso.\r\n",
    "- `ReduceLROnPlateau`: Un programador de tasa de aprendizaje que reduce la tasa de aprendizaje cuando una métrica ha dejado de mejorar.\r\n",
    "- `mse_loss`, `l1_loss`: Funciones de pérdida Mean Squared Error (MSE) y Mean Absolute Error (L1).\r\n",
    "- `Tensor`: Tipo de datos de tensor de PyTorch.\r\n",
    "- `Optional`, `Dict`, `Tuple`: Tipos de datos de Python utilizados para definir tipos de argumentos y retornos de funciones.\r\n",
    "- `LightningModule`: Clase base proporcionada por PyTorch Lightning para la creación de módulos de red neuronal.\r\n",
    "- `create_model`, `load_model`: Funciones de un módulo llamado `torchmdnet` para crear y cargar modelos.\r\n",
    "\r\n",
    "Estas importaciones son esenciales para las funcionalidades posteriores del módulo.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bde3c-7ea3-407b-b7e0-65db568e8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.functional import mse_loss, l1_loss\n",
    "from torch import Tensor\n",
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmdnet.models.model import create_model, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5f0a2-479c-4d4b-8eac-08a6f1d5da7d",
   "metadata": {},
   "source": [
    "## Definición de la clase `LNNP`\r\n",
    "\r\n",
    "Creamos una clase llamada `LNNP` que hereda de `LightningModule`. Esta clase sirve como un contenedor para nuestra red neuronal potencial (NNP, por sus siglas en inglés) implementada en TorchMD-Net. La documentación de la clase es la siguiente:\r\n",
    "\r\n",
    "- **Nombre**: `LNNP`\r\n",
    "- **Hereda de**: `LightningModule`\r\n",
    "- **Descripción**: Se trata de un contenedor que envuelve las funcionalidades de la red neuronal potencial (NNP) implementada en TorchMD-Net utilizando PyTorch Lightning. Esta clase nos permite entrenar, validar y probar la NNP de manera eficiente con las capacidades adicionales proporcionadas por PyTorch Lightning.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358cd16d-5f4d-409b-88d4-806120ad0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNNP(LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning wrapper for the Neural Network Potentials in TorchMD-Net.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12114112-406c-49e7-ae44-80d4a5a1484e",
   "metadata": {},
   "source": [
    "## Constructor `__init__`\r\n",
    "\r\n",
    "Definimos el constructor `__init__` de la clase `LNNP`. Este método se llama automáticamente cuando se crea una instancia de la clase `LNNP`. Aquí está lo que hace:\r\n",
    "\r\n",
    "- **Argumentos**:\r\n",
    "  - `hparams`: Un diccionario que contiene hiperparámetros para configurar el modelo.\r\n",
    "  - `prior_model`: Un modelo previo opcional que se puede utilizar para inicializar el modelo.\r\n",
    "  - `mean`, `std`: Medias y desviaciones estándar opcionales para normalizar los datos de entrada.\r\n",
    "- **Superllamada**: Llamamos al constructor de la clase base (`LightningModule`) usando `super()`, asegurándonos de inicializar todos los atributos y métodos heredados.\r\n",
    "- **Ajuste de hiperparámetros**: Añadimos claves faltantes (\"charge\" y \"spin\") al diccionario de hiperparámetros `hparams` si no están presentes. Luego, guardamos todos los hiperparámetros utilizando `save_hyperparameters(hparams)`, lo que permite el acceso a los hiperparámetros en todo el módulo con `self.hparams`.\r\n",
    "- **Inicialización del modelo**: Si se especifica un modelo para cargar (`load_model` en los hiperparámetros), cargamos ese modelo utilizando la función `load_model`. De lo contrario, creamos un nuevo modelo utilizando la función `create_model`, pasando los hiperparámetros y cualquier modelo previo, media y desviación estándar si están disponibles.\r\n",
    "- **Inicialización de la suavización exponencial**: Inicializamos una variable `ema` (Exponential Moving Average) y reiniciamos un diccionario asociado utilizando `_reset_ema_dict()`.\r\n",
    "- **Inicialización de la colección de pérdidas**: Inicializamos una variable `losses` y reiniciamos un diccionario asociado utilizando `_reset_losses_dict()`.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1d588-efb8-421d-a703-329a47efac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, hparams, prior_model=None, mean=None, std=None):\n",
    "        super(LNNP, self).__init__()\n",
    "\n",
    "        if \"charge\" not in hparams:\n",
    "            hparams[\"charge\"] = False\n",
    "        if \"spin\" not in hparams:\n",
    "            hparams[\"spin\"] = False\n",
    "\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        if self.hparams.load_model:\n",
    "            self.model = load_model(self.hparams.load_model, args=self.hparams)\n",
    "        else:\n",
    "            self.model = create_model(self.hparams, prior_model, mean, std)\n",
    "\n",
    "        # initialize exponential smoothing\n",
    "        self.ema = None\n",
    "        self._reset_ema_dict()\n",
    "\n",
    "        # initialize loss collection\n",
    "        self.losses = None\n",
    "        self._reset_losses_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92fe7fc-da6b-4aaf-a09b-235f2d0f26a5",
   "metadata": {},
   "source": [
    "## Configuración de los optimizadores\r\n",
    "\r\n",
    "Definimos el método `configure_optimizers`, que se encarga de configurar los optimizadores y los planificadores de tasas de aprendizaje para el entrenamiento. Aquí está lo que hace:\r\n",
    "\r\n",
    "- **Optimizador**: Utilizamos el optimizador AdamW para optimizar los parámetros del modelo. Configuramos el optimizador con los siguientes argumentos:\r\n",
    "  - `parameters()`: Los parámetros del modelo que se optimizarán.\r\n",
    "  - `lr`: Tasa de aprendizaje, tomada de los hiperparámetros (`self.hparams.lr`).\r\n",
    "  - `weight_decay`: Término de regularización, tomado de los hiperparámetros (`self.hparams.weight_decay`).\r\n",
    "- **Planificador de tasas de aprendizaje**: Utilizamos `ReduceLROnPlateau` para ajustar la tasa de aprendizaje dinámicamente durante el entrenamiento. Configuramos el planificador con los siguientes argumentos:\r\n",
    "  - `optimizer`: El optimizador que estamos utilizando.\r\n",
    "  - `mode`: El modo de reducción de la tasa de aprendizaje, en este caso, \"min\" para reducir la tasa de aprendizaje cuando la métrica monitoreada deja de disminuir.\r\n",
    "  - `factor`, `patience`, `min_lr`: Factores de reducción, paciencia antes de reducir y el mínimo de la tasa de aprendizaje permitido, tomados de los hiperparámetros (`self.hparams.lr_factor`, `self.hparams.lr_patience`, `self.hparams.lr_min`).\r\n",
    "- **Configuración del planificador de tasas de aprendizaje**: Creamos un diccionario `lr_scheduler` que contiene la configuración del planificador de tasas de aprendizaje. Esto incluye el planificador mismo, la métrica que se está monitoreando, el intervalo en el que se aplicará el planificador y la frecuencia con la que se llamará, todo tomado de los hiperparámetros.\r\n",
    "- **Retorno**: Devolvemos una lista de optimizadores y una lista de planificadores de tasas de aprendizaje para ser utilizados durante el entrenamiento.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84194cd7-a386-4700-a893-a93d4da58b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            \"min\",\n",
    "            factor=self.hparams.lr_factor,\n",
    "            patience=self.hparams.lr_patience,\n",
    "            min_lr=self.hparams.lr_min,\n",
    "        )\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"monitor\": getattr(self.hparams, \"lr_metric\", \"val_loss\"),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f90c4b-1a28-448f-a110-73491dc03ed0",
   "metadata": {},
   "source": [
    "## Método `forward`\n",
    "\n",
    "Definimos el método `forward`, que se encarga de pasar los datos a través del modelo. Aquí está lo que hace:\n",
    "\n",
    "- **Argumentos**:\n",
    "  - `z`: Tensor de características de entrada.\n",
    "  - `pos`: Tensor de posiciones de los átomos.\n",
    "  - `batch`: Tensor opcional que indica la pertenencia de cada átomo a un batch específico.\n",
    "  - `q`: Tensor opcional de cargas de átomos.\n",
    "  - `s`: Tensor opcional de spin de los átomos.\n",
    "  - `extra_args`: Argumentos adicionales opcionales.\n",
    "- **Retorno**:\n",
    "  - Una tupla que contiene:\n",
    "    - Tensor de salida del modelo.\n",
    "    - Tensor opcional adicional (puede ser `None`).\n",
    "\n",
    "Dentro del método, llamamos al método `forward` del modelo subyacente (`self.model`) pasando los argumentos proporcionados. Esto permite que los datos fluyan a través del modelo y obtengamos la salida correspondiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231eeb4-d101-413c-901a-2be93ee6f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self,\n",
    "                z: Tensor,\n",
    "                pos: Tensor,\n",
    "                batch: Optional[Tensor] = None,\n",
    "                q: Optional[Tensor] = None,\n",
    "                s: Optional[Tensor] = None,\n",
    "                extra_args: Optional[Dict[str, Tensor]] = None\n",
    "                ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "        return self.model(z, pos, batch=batch, q=q, s=s, extra_args=extra_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76c34e-59d4-4540-b23a-ded6bcf0d05e",
   "metadata": {},
   "source": [
    "## Método `training_step`\r\n",
    "\r\n",
    "Definimos el método `training_step`, que se encarga de realizar un paso de entrenamiento durante el entrenamiento del modelo. Aquí está lo que hace:\r\n",
    "\r\n",
    "- **Argumentos**:\r\n",
    "  - `batch`: Un lote de datos de entrenamiento.\r\n",
    "  - `batch_idx`: El índice del lote actual.\r\n",
    "- **Retorno**:\r\n",
    "  - El resultado del método `step`, que contiene la pérdida calculada durante el paso de entrenamiento.\r\n",
    "\r\n",
    "Dentro del método, llamamos a un método interno llamado `step`, pasando el lote de datos de entrenamiento, la función de pérdida MSE (`mse_loss`) y la cadena \"train\" para indicar que estamos en la fase de entrenamiento. El método `step` se encarga de realizar el cálculo de la pérdida y cualquier otro procesamiento necesario para un paso de entrenamiento.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d7e7f-dfb6-498a-ab10-8e40df550803",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, mse_loss, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da2416-a4cc-4431-b53c-9f0755ca0f6e",
   "metadata": {},
   "source": [
    "## Método `validation_step`\n",
    "\n",
    "Definimos el método `validation_step`, que se encarga de realizar un paso de validación durante el entrenamiento del modelo. Aquí está lo que hace:\n",
    "\n",
    "- **Argumentos**:\n",
    "  - `batch`: Un lote de datos de validación.\n",
    "  - `batch_idx`: El índice del lote actual.\n",
    "  - `*args`: Argumentos adicionales, se utiliza para determinar si estamos en la fase de prueba o validación.\n",
    "- **Retorno**:\n",
    "  - El resultado del método `step`, que contiene la pérdida calculada durante el paso de validación o prueba.\n",
    "\n",
    "Dentro del método, verificamos la longitud de los argumentos y su valor para determinar si estamos en la fase de validación o prueba. Si no se proporcionan argumentos adicionales o si el primer argumento es cero, realizamos un paso de validación utilizando la función de pérdida MSE (`mse_loss`). Si el primer argumento es diferente de cero, realizamos un paso de prueba utilizando la función de pérdida L1 (`l1_loss`). En ambos casos, llamamos al método `step` para realizar el cálculo de la pérdida y cualquier otro procesamiento necesario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76083bd-8e58-48ef-a316-0a08e059f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def validation_step(self, batch, batch_idx, *args):\n",
    "        if len(args) == 0 or (len(args) > 0 and args[0] == 0):\n",
    "            # validation step\n",
    "            return self.step(batch, mse_loss, \"val\")\n",
    "        # test step\n",
    "        return self.step(batch, l1_loss, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f9612-0a4e-4539-ba64-415524f789e7",
   "metadata": {},
   "source": [
    "## Método `test_step`\n",
    "\n",
    "Definimos el método `test_step`, que se encarga de realizar un paso de prueba durante la evaluación del modelo. Aquí está lo que hace:\n",
    "\n",
    "- **Argumentos**:\n",
    "  - `batch`: Un lote de datos de prueba.\n",
    "  - `batch_idx`: El índice del lote actual.\n",
    "- **Retorno**:\n",
    "  - El resultado del método `step`, que contiene la pérdida calculada durante el paso de prueba.\n",
    "\n",
    "Dentro del método, llamamos al método `step`, pasando el lote de datos de prueba, la función de pérdida L1 (`l1_loss`) y la cadena \"test\" para indicar que estamos en la fase de prueba. El método `step` se encarga de realizar el cálculo de la pérdida y cualquier otro procesamiento necesario para un paso de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930960b-5f3b-419c-872e-fff36644495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, l1_loss, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d3628-025b-4833-b25e-87ec9978d49d",
   "metadata": {},
   "source": [
    "## Método `step`\n",
    "\n",
    "Definimos el método `step`, que encapsula el procesamiento común para un paso de entrenamiento, validación o prueba. Aquí está lo que hace:\n",
    "\n",
    "- **Argumentos**:\n",
    "  - `batch`: Un lote de datos.\n",
    "  - `loss_fn`: La función de pérdida a utilizar para calcular la pérdida.\n",
    "  - `stage`: Una cadena que indica la etapa actual, que puede ser \"train\" (entrenamiento), \"val\" (validación) o \"test\" (prueba).\n",
    "- **Retorno**:\n",
    "  - La pérdida calculada para el lote actual.\n",
    "\n",
    "Dentro del método, realizamos los siguientes pasos:\n",
    "\n",
    "1. **Gestión de los gradientes**: Configuramos el entorno de gradiente según la etapa actual (`train`) o si se requiere el cálculo del gradiente (`derivative` en los hiperparámetros).\n",
    "2. **Obtención de la salida del modelo**: Llamamos al método `forward` del modelo para obtener las predicciones (`y`) y las negativas de las derivadas (`neg_dy`) utilizando los datos del lote.\n",
    "3. **Cálculo de las pérdidas**: Calculamos las pérdidas para las predicciones (`y`) y las negativas de las derivadas (`neg_dy`) si están disponibles en el lote de datos. Si se especifica, aplicamos suavizado exponencial a las pérdidas durante el entrenamiento y la validación. Luego, calculamos la pérdida total como la suma ponderada de las pérdidas de las predicciones y las negativas de las derivadas.\n",
    "4. **Almacenamiento de las pérdidas**: Almacenamos las pérdidas calculadas en la lista de pérdidas correspondiente a la etapa actual (`stage`).\n",
    "5. **Retorno de la pérdida**: Devolvemos la pérdida total calculada.\n",
    "\n",
    "Este método encapsula el cálculo de la pérdida y el procesamiento asociado para un paso de entrenamiento, validación o prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eefc20-8b0b-4a40-83f4-549dc98d1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, batch, loss_fn, stage):\n",
    "        with torch.set_grad_enabled(stage == \"train\" or self.hparams.derivative):\n",
    "            extra_args = batch.to_dict()\n",
    "            for a in ('y', 'neg_dy', 'z', 'pos', 'batch', 'q', 's'):\n",
    "                if a in extra_args:\n",
    "                    del extra_args[a]\n",
    "            # TODO: the model doesn't necessarily need to return a derivative once\n",
    "            # Union typing works under TorchScript (https://github.com/pytorch/pytorch/pull/53180)\n",
    "            y, neg_dy = self(\n",
    "                batch.z,\n",
    "                batch.pos,\n",
    "                batch=batch.batch,\n",
    "                q=batch.q if self.hparams.charge else None,\n",
    "                s=batch.s if self.hparams.spin else None,\n",
    "                extra_args=extra_args\n",
    "            )\n",
    "\n",
    "        loss_y, loss_neg_dy = 0, 0\n",
    "        if self.hparams.derivative:\n",
    "            if \"y\" not in batch:\n",
    "                # \"use\" both outputs of the model's forward function but discard the first\n",
    "                # to only use the negative derivative and avoid 'Expected to have finished reduction\n",
    "                # in the prior iteration before starting a new one.', which otherwise get's\n",
    "                # thrown because of setting 'find_unused_parameters=False' in the DDPPlugin\n",
    "                neg_dy = neg_dy + y.sum() * 0\n",
    "\n",
    "            # negative derivative loss\n",
    "            loss_neg_dy = loss_fn(neg_dy, batch.neg_dy)\n",
    "\n",
    "            if stage in [\"train\", \"val\"] and self.hparams.ema_alpha_neg_dy < 1:\n",
    "                if self.ema[stage + \"_neg_dy\"] is None:\n",
    "                    self.ema[stage + \"_neg_dy\"] = loss_neg_dy.detach()\n",
    "                # apply exponential smoothing over batches to neg_dy\n",
    "                loss_neg_dy = (\n",
    "                    self.hparams.ema_alpha_neg_dy * loss_neg_dy\n",
    "                    + (1 - self.hparams.ema_alpha_neg_dy) * self.ema[stage + \"_neg_dy\"]\n",
    "                )\n",
    "                self.ema[stage + \"_neg_dy\"] = loss_neg_dy.detach()\n",
    "\n",
    "            if self.hparams.neg_dy_weight > 0:\n",
    "                self.losses[stage + \"_neg_dy\"].append(loss_neg_dy.detach())\n",
    "\n",
    "        if \"y\" in batch:\n",
    "            if batch.y.ndim == 1:\n",
    "                batch.y = batch.y.unsqueeze(1)\n",
    "\n",
    "            # y loss\n",
    "            loss_y = loss_fn(y, batch.y)\n",
    "\n",
    "            if stage in [\"train\", \"val\"] and self.hparams.ema_alpha_y < 1:\n",
    "                if self.ema[stage + \"_y\"] is None:\n",
    "                    self.ema[stage + \"_y\"] = loss_y.detach()\n",
    "                # apply exponential smoothing over batches to y\n",
    "                loss_y = (\n",
    "                    self.hparams.ema_alpha_y * loss_y\n",
    "                    + (1 - self.hparams.ema_alpha_y) * self.ema[stage + \"_y\"]\n",
    "                )\n",
    "                self.ema[stage + \"_y\"] = loss_y.detach()\n",
    "\n",
    "            if self.hparams.y_weight > 0:\n",
    "                self.losses[stage + \"_y\"].append(loss_y.detach())\n",
    "\n",
    "        # total loss\n",
    "        loss = loss_y * self.hparams.y_weight + loss_neg_dy * self.hparams.neg_dy_weight\n",
    "\n",
    "        self.losses[stage].append(loss.detach())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca09d9-7761-464c-a558-4f7a76906405",
   "metadata": {},
   "source": [
    "## Método `optimizer_step`\n",
    "\n",
    "Definimos el método `optimizer_step`, que se encarga de realizar un paso de optimización durante el entrenamiento del modelo. Aquí está lo que hace:\n",
    "\n",
    "- **Argumentos**:\n",
    "  - `*args`, `**kwargs`: Argumentos posicionales y de palabras clave que pueden ser pasados al método. En particular, esperamos encontrar el optimizador en `kwargs[\"optimizer\"]` o en `args[2]`.\n",
    "- **Procedimiento**:\n",
    "  - **Escalado de tasa de aprendizaje durante el calentamiento**: Si el paso global del entrenamiento es menor que el número de pasos de calentamiento especificado en los hiperparámetros (`lr_warmup_steps`), escalamos la tasa de aprendizaje del optimizador linealmente desde 0 hasta la tasa de aprendizaje especificada. Esto se hace para suavizar el proceso de inicio del entrenamiento y evitar grandes actualizaciones de los pesos del modelo al principio.\n",
    "  - **Actualización del optimizador**: Llamamos al método `optimizer_step` de la clase base para realizar la actualización real de los parámetros del modelo utilizando el optimizador proporcionado.\n",
    "  - **Reinicio de los gradientes**: Después de realizar la actualización de los parámetros del modelo, reiniciamos los gradientes del optimizador utilizando `optimizer.zero_grad()` para prepararlos para el próximo paso de optimización.\n",
    "\n",
    "Este método encapsula el proceso de optimización, incluyendo el escalado de la tasa de aprendizaje durante el calentamiento y la actualización de los parámetros del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39bcf3-0e8b-4b53-ac63-4140aa9f80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        optimizer = kwargs[\"optimizer\"] if \"optimizer\" in kwargs else args[2]\n",
    "        if self.trainer.global_step < self.hparams.lr_warmup_steps:\n",
    "            lr_scale = min(\n",
    "                1.0,\n",
    "                float(self.trainer.global_step + 1)\n",
    "                / float(self.hparams.lr_warmup_steps),\n",
    "            )\n",
    "\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] = lr_scale * self.hparams.lr\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdea1f-dbdf-41db-ad01-8ee2e28ec353",
   "metadata": {},
   "source": [
    "## Método `training_epoch_end`\r\n",
    "\r\n",
    "Definimos el método `training_epoch_end`, que se ejecuta al final de cada época de entrenamiento. Aquí está lo que hace:\r\n",
    "\r\n",
    "- **Argumentos**:\r\n",
    "  - `training_step_outputs`: Una lista de los resultados de los pasos de entrenamiento durante la época.\r\n",
    "- **Procedimiento**:\r\n",
    "  - **Obtención del datamodule**: Recuperamos el datamodule del entrenador (`self.trainer.datamodule`).\r\n",
    "  - **Comprobación de la existencia de un conjunto de datos de prueba**: Verificamos si el datamodule tiene un atributo `test_dataset` y si contiene datos.\r\n",
    "  - **Reinicio del dataloader de validación**: Si se cumplen ciertas condiciones, reiniciamos el dataloader de validación para el módulo actual. Las condiciones incluyen si el número de época actual es divisible por el intervalo de prueba especificado en los hiperparámetros (`test_interval`) o si el siguiente número de época también es divisible por este intervalo. El reinicio del dataloader de validación se realiza para preparar el dataloader para la evaluación del conjunto de datos de prueba, lo que puede ser más rápido que omitir los pasos de evaluación de prueba devolviendo `None`.\r\n",
    "\r\n",
    "Este método permite la configuración necesaria antes y después de la evaluación del conjunto de datos de prueba al final de cada época de entrenamiento.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c299d7-38d7-4b96-96e2-1698042b216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        dm = self.trainer.datamodule\n",
    "        if hasattr(dm, \"test_dataset\") and len(dm.test_dataset) > 0:\n",
    "            should_reset = (\n",
    "                self.current_epoch % self.hparams.test_interval == 0\n",
    "                or (self.current_epoch + 1) % self.hparams.test_interval == 0\n",
    "            )\n",
    "            if should_reset:\n",
    "                # reset validation dataloaders before and after testing epoch, which is faster\n",
    "                # than skipping test validation steps by returning None\n",
    "                self.trainer.reset_val_dataloader(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fd95e-eae8-46d0-b601-fe1df6b88398",
   "metadata": {},
   "source": [
    "## Método `validation_epoch_end`\r\n",
    "\r\n",
    "Definimos el método `validation_epoch_end`, que se ejecuta al final de cada época de validación. Aquí está lo que hace:\r\n",
    "\r\n",
    "- **Argumentos**:\r\n",
    "  - `validation_step_outputs`: Una lista de los resultados de los pasos de validación durante la época.\r\n",
    "- **Procedimiento**:\r\n",
    "  - **Verificación del modo de chequeo de cordura**: Verificamos si el entrenamiento está en modo de chequeo de cordura (`sanity_checking`). Si no lo está, continuamos con el proceso de registro de métricas.\r\n",
    "  - **Construcción del diccionario de métricas registradas**: Creamos un diccionario (`result_dict`) que contiene métricas importantes para el entrenamiento y la validación, como el número de época actual, la tasa de aprendizaje, la pérdida promedio de entrenamiento (`train_loss`) y la pérdida promedio de validación (`val_loss`). Si hay pérdidas para el conjunto de datos de prueba (`test_loss`), también se agregan al diccionario.\r\n",
    "  - **Registro de las métricas**: Registramos las métricas utilizando el método `log_dict`, que sincroniza la información en todos los dispositivos en caso de entrenamiento distribuido.\r\n",
    "  - **Reinicio de las pérdidas registradas**: Después de registrar las métricas, reiniciamos las listas de pérdidas para la próxima época.\r\n",
    "\r\n",
    "Este método permite registrar y registrar las métricas importantes al final de cada época de validación, lo que ayuda a monitorear el rendimiento del modelo durante el entrenamiento.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087a1a8-38a2-4481-ad61-c9366d117d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        if not self.trainer.sanity_checking:\n",
    "            # construct dict of logged metrics\n",
    "            result_dict = {\n",
    "                \"epoch\": float(self.current_epoch),\n",
    "                \"lr\": self.trainer.optimizers[0].param_groups[0][\"lr\"],\n",
    "                \"train_loss\": torch.stack(self.losses[\"train\"]).mean(),\n",
    "                \"val_loss\": torch.stack(self.losses[\"val\"]).mean(),\n",
    "            }\n",
    "\n",
    "            # add test loss if available\n",
    "            if len(self.losses[\"test\"]) > 0:\n",
    "                result_dict[\"test_loss\"] = torch.stack(self.losses[\"test\"]).mean()\n",
    "\n",
    "            # if prediction and derivative are present, also log them separately\n",
    "            if len(self.losses[\"train_y\"]) > 0 and len(self.losses[\"train_neg_dy\"]) > 0:\n",
    "                result_dict[\"train_loss_y\"] = torch.stack(self.losses[\"train_y\"]).mean()\n",
    "                result_dict[\"train_loss_neg_dy\"] = torch.stack(\n",
    "                    self.losses[\"train_neg_dy\"]\n",
    "                ).mean()\n",
    "                result_dict[\"val_loss_y\"] = torch.stack(self.losses[\"val_y\"]).mean()\n",
    "                result_dict[\"val_loss_neg_dy\"] = torch.stack(self.losses[\"val_neg_dy\"]).mean()\n",
    "\n",
    "                if len(self.losses[\"test\"]) > 0:\n",
    "                    result_dict[\"test_loss_y\"] = torch.stack(\n",
    "                        self.losses[\"test_y\"]\n",
    "                    ).mean()\n",
    "                    result_dict[\"test_loss_neg_dy\"] = torch.stack(\n",
    "                        self.losses[\"test_neg_dy\"]\n",
    "                    ).mean()\n",
    "\n",
    "            self.log_dict(result_dict, sync_dist=True)\n",
    "        self._reset_losses_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b0f599-10dd-4e4d-ad86-b49c37522ec3",
   "metadata": {},
   "source": [
    "## Método `_reset_losses_dict`\r\n",
    "\r\n",
    "Definimos el método `_reset_losses_dict`, que se encarga de reiniciar el diccionario de pérdidas (`self.losses`). Aquí está lo que hace:\r\n",
    "\r\n",
    "- **Procedimiento**:\r\n",
    "  - **Reinicio del diccionario de pérdidas**: Creamos un nuevo diccionario llamado `self.losses` que contiene listas vacías para diferentes tipos de pérdidas, como pérdidas de entrenamiento (`train`), pérdidas de validación (`val`), pérdidas de prueba (`test`), pérdidas de predicción (`train_y`, `val_y`, `test_y`) y pérdidas de derivada negativa (`train_neg_dy`, `val_neg_dy`, `test_neg_dy`).\r\n",
    "\r\n",
    "Este método se utiliza para reiniciar el registro de las pérdidas al comienzo de cada época, lo que garantiza que las pérdidas de la época anterior no se acumulen en las métricas registradas.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf1f90-4630-4ac6-aff3-f6bad8f135b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _reset_losses_dict(self):\n",
    "        self.losses = {\n",
    "            \"train\": [],\n",
    "            \"val\": [],\n",
    "            \"test\": [],\n",
    "            \"train_y\": [],\n",
    "            \"val_y\": [],\n",
    "            \"test_y\": [],\n",
    "            \"train_neg_dy\": [],\n",
    "            \"val_neg_dy\": [],\n",
    "            \"test_neg_dy\": [],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04fa0f-635a-4574-ba1d-6a27f0906957",
   "metadata": {},
   "source": [
    "## Método `_reset_ema_dict`\n",
    "\n",
    "Definimos el método `_reset_ema_dict`, que se encarga de reiniciar el diccionario de suavizado exponencial móvil (EMA, por sus siglas en inglés). Aquí está lo que hace:\n",
    "\n",
    "- **Procedimiento**:\n",
    "  - **Reinicio del diccionario de EMA**: Creamos un nuevo diccionario llamado `self.ema` que contiene valores `None` para diferentes tipos de EMA, como EMA de pérdidas de predicción durante el entrenamiento (`train_y`), EMA de pérdidas de predicción durante la validación (`val_y`), EMA de pérdidas de derivada negativa durante el entrenamiento (`train_neg_dy`) y EMA de pérdidas de derivada negativa durante la validación (`val_neg_dy`).\n",
    "\n",
    "Este método se utiliza para reiniciar el seguimiento del suavizado exponencial móvil al comienzo de cada entrenamiento y validación, lo que garantiza que los valores de EMA se calculen correctamente para cada época.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285586c-1888-4ddc-ba9f-0c264e93e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _reset_ema_dict(self):\n",
    "        self.ema = {\"train_y\": None, \"val_y\": None, \"train_neg_dy\": None, \"val_neg_dy\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288dfb4-1f41-42ab-acd8-8dc8ada61200",
   "metadata": {},
   "source": [
    "# Ejemplo de implementación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7db190-dfc5-4fc8-9142-5b4c581ec5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir hiperparámetros\r\n",
    "hparams = {\r\n",
    "    \"lr\": 0.001,\r\n",
    "    \"weight_decay\": 0.0001,\r\n",
    "    \"lr_factor\": 0.1,\r\n",
    "    \"lr_patience\": 10,\r\n",
    "    \"lr_min\": 1e-6,\r\n",
    "    \"charge\": False,\r\n",
    "    \"spin\": False,\r\n",
    "    \"derivative\": True,\r\n",
    "    \"ema_alpha_neg_dy\": 0.99,\r\n",
    "    \"ema_alpha_y\": 0.99,\r\n",
    "    \"neg_dy_weight\": 0.5,\r\n",
    "    \"y_weight\": 0.5,\r\n",
    "    \"lr_warmup_steps\": 1000,\r\n",
    "    \"test_interval\": 5\r\n",
    "}\r\n",
    "\r\n",
    "# Crear instancia de la clase LNNP\r\n",
    "model = LNNP(hparams)\r\n",
    "\r\n",
    "# Cargar conjunto de datos (MNIST como ejemplo)\r\n",
    "train_dataset = MNIST(root='data/', train=True, download=True, transform=ToTensor())\r\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\r\n",
    "\r\n",
    "# Crear logger para visualizar en TensorBoard\r\n",
    "logger = TensorBoardLogger(\"logs\", name=\"LNNP\")\r\n",
    "\r\n",
    "# Entrenar el modelo\r\n",
    "trainer = Trainer(logger=logger, max_epochs=10)\r\n",
    "trainer.fit(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
