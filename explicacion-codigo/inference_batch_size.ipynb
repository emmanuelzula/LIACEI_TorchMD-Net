{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f07f2a-7d8a-492f-944d-f036d7d4726b",
   "metadata": {},
   "source": [
    "# Funcionamiento del parametro inference_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06181514-384a-47d5-943f-f39d99dea1aa",
   "metadata": {},
   "source": [
    "## torchmdnet/data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0b86a-3bab-4487-b303-2e7e068b2595",
   "metadata": {},
   "source": [
    "### Configuración del Entorno y Importación de Bibliotecas\r\n",
    "\r\n",
    "Para comenzar, importamos las bibliotecas necesarias para nuestro módulo, que incluyen:\r\n",
    "\r\n",
    "- `os.path.join`: Utilizado para unir diferentes partes de una ruta de archivo.\r\n",
    "- `tqdm`: Proporciona una barra de progreso visual para iteraciones.\r\n",
    "- `torch`: La biblioteca principal de PyTorch.\r\n",
    "- `torch.utils.data.Subset`: Permite crear un subconjunto de un conjunto de datos.\r\n",
    "- `torch_geometric.loader.DataLoader`: DataLoader específico para datos en formato geométrico en PyTorch.\r\n",
    "- `pytorch_lightning.LightningDataModule`: Clase base para definir los módulos de datos en PyTorch Lightning.\r\n",
    "- `pytorch_lightning.utilities.rank_zero_warn`: Utilizado para mostrar advertencias solo en el proceso de rango cero.\r\n",
    "- `torchmdnet.datasets`: Importa los conjuntos de datos necesarios para nuestro módulo.\r\n",
    "- `torch_geometric.data.Dataset`: Clase base para definir conjuntos de datos en formato geométrico en PyTorch.\r\n",
    "- `torchmdnet.utils.make_splits`: Función para dividir un conjunto de datos en conjuntos de entrenamiento, validación y prueba.\r\n",
    "- `torchmdnet.utils.MissingEnergyException`: Excepción personalizada para manejar casos en los que falte energía en el conjunto de datos.\r\n",
    "- `torch_scatter.scatter`: Función para realizar operaciones de reducción (scatter) en datos dispersos.\r\n",
    "\r\n",
    "Además, importamos `dtype_mapping` de `torchmdnet.models.utils`, que probablemente sea un mapeo de tipos de datos utilizado en los modelos.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c781ac-ce79-4591-b8bd-3ffbd7bfe348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning.utilities import rank_zero_warn\n",
    "from torchmdnet import datasets\n",
    "from torch_geometric.data import Dataset\n",
    "from torchmdnet.utils import make_splits, MissingEnergyException\n",
    "from torch_scatter import scatter\n",
    "from torchmdnet.models.utils import dtype_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeda6ad-5ad2-4d68-a681-6d9e27cdc2f6",
   "metadata": {},
   "source": [
    "### Clase `FloatCastDatasetWrapper`\r\n",
    "\r\n",
    "Esta clase es una envoltura (wrapper) alrededor de un conjunto de datos existente y tiene la intención de convertir los datos a un tipo de dato de punto flotante específico. Aquí está una descripción de sus componentes:\r\n",
    "\r\n",
    "- `FloatCastDatasetWrapper(Dataset)`: Esta clase hereda de `torch_geometric.data.Dataset`, lo que significa que actúa como un tipo de conjunto de datos en formato geométrico en PyTorch.\r\n",
    "\r\n",
    "- `__init__(self, dataset, dtype=torch.float64)`: El método de inicialización de la clase toma dos argumentos: `dataset`, que es el conjunto de datos que se envuelve, y `dtype`, que es el tipo de dato al que se deben convertir los datos. Por defecto, el tipo de dato es `torch.float64`.\r\n",
    "\r\n",
    "- `super(FloatCastDatasetWrapper, self).__init__(dataset.root, dataset.transform, dataset.pre_transform, dataset.pre_filter)`: Llama al constructor de la clase base `Dataset` con los mismos argumentos que el conjunto de datos original, es decir, `root`, `transform`, `pre_transform`, y `pre_filter`.\r\n",
    "\r\n",
    "- `self.dataset = dataset`: Almacena una referencia al conjunto de datos original.\r\n",
    "\r\n",
    "- `self.dtype = dtype`: Almacena el tipo de dato al que se deben convertir los datos.\r\n",
    "\r\n",
    "Este wrapper permite mantener la funcionalidad del conjunto de datos original mientras se realiza una conversión de tipo de dato a flotante. Esto puede ser útil para asegurar la consistencia en el tipo de dato de entrada para modelos de aprendizaje automático.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5de21c-decb-4783-bf1d-1271693f12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatCastDatasetWrapper(Dataset):\n",
    "    def __init__(self, dataset, dtype=torch.float64):\n",
    "        super(FloatCastDatasetWrapper, self).__init__(\n",
    "            dataset.root, dataset.transform, dataset.pre_transform, dataset.pre_filter\n",
    "        )\n",
    "        self.dataset = dataset\n",
    "        self.dtype = dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed0ff3-901f-44ba-8654-b729b5e44b70",
   "metadata": {},
   "source": [
    "### Método `len`\n",
    "\n",
    "Este método sobrecarga la función `len` para devolver la longitud del conjunto de datos envuelto por la instancia de `FloatCastDatasetWrapper`. Aquí está una descripción de su funcionalidad:\n",
    "\n",
    "- `def len(self)`: Este método define una función llamada `len` que toma `self` como argumento, lo que significa que opera en una instancia de la clase `FloatCastDatasetWrapper`.\n",
    "\n",
    "- `return len(self.dataset)`: Dentro del método, se llama a la función `len` en el conjunto de datos original (`self.dataset`) y se devuelve su longitud. Esto permite que la instancia de `FloatCastDatasetWrapper` se comporte como un conjunto de datos estándar en términos de su longitud, proporcionando una interfaz consistente para su uso en iteraciones y otras operaciones que dependen de conocer la longitud del conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d05f2-a167-4d60-9654-d78a85360ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def len(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599bd0ac-2399-4ff4-bad8-f02e799bea21",
   "metadata": {},
   "source": [
    "### Método `get`\n",
    "\n",
    "Este método se utiliza para obtener un dato del conjunto de datos envuelto por la instancia de `FloatCastDatasetWrapper` y convertir los tensores a un tipo de dato de punto flotante específico. Aquí está una descripción de su funcionalidad:\n",
    "\n",
    "- `def get(self, idx)`: Define un método llamado `get` que toma `idx` como argumento, indicando el índice del dato que se quiere obtener del conjunto de datos envuelto.\n",
    "\n",
    "- `data = self.dataset.get(idx)`: Se llama al método `get` en el conjunto de datos original (`self.dataset`) para obtener el dato correspondiente al índice `idx`.\n",
    "\n",
    "- `for key, value in data`: Se itera sobre los pares clave-valor en el dato obtenido.\n",
    "\n",
    "- `if torch.is_tensor(value) and torch.is_floating_point(value)`: Se comprueba si el valor es un tensor de punto flotante.\n",
    "\n",
    "- `setattr(data, key, value.to(self.dtype))`: Si el valor es un tensor de punto flotante, se convierte a `self.dtype` (el tipo de dato especificado) y se establece como atributo en el dato.\n",
    "\n",
    "- `return data`: Se devuelve el dato modificado, donde los tensores han sido convertidos al tipo de dato especificado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb4b06-92a0-4035-8ae4-568f4888981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get(self, idx):\n",
    "        data = self.dataset.get(idx)\n",
    "        for key, value in data:\n",
    "            if torch.is_tensor(value) and torch.is_floating_point(value):\n",
    "                setattr(data, key, value.to(self.dtype))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee938270-2165-4280-bcae-a25eae72c5f6",
   "metadata": {},
   "source": [
    "### Método `__getattr__`\n",
    "\n",
    "Este método se utiliza para interceptar y manejar la obtención de atributos que no están definidos explícitamente en la clase `FloatCastDatasetWrapper`. Aquí está una descripción de su funcionalidad:\n",
    "\n",
    "- `def __getattr__(self, name)`: Define un método especial llamado `__getattr__` que se activa cuando se intenta acceder a un atributo que no está definido explícitamente en la clase.\n",
    "\n",
    "- `if hasattr(self.dataset, name)`: Verifica si el atributo existe en el conjunto de datos subyacente (`self.dataset`).\n",
    "\n",
    "- `return getattr(self.dataset, name)`: Si el atributo existe en el conjunto de datos subyacente, devuelve el valor del atributo llamando a `getattr` en el conjunto de datos.\n",
    "\n",
    "- `raise AttributeError(...)`: Si el atributo no existe ni en la clase `FloatCastDatasetWrapper` ni en su conjunto de datos subyacente, se genera una excepción `AttributeError` que indica que el atributo no se puede encontrar.\n",
    "\n",
    "Este método permite que la clase `FloatCastDatasetWrapper` delegue la obtención de atributos a su conjunto de datos subyacente, lo que garantiza que pueda acceder a todos los atributos y métodos del conjunto de datos original sin tener que definirlos nuevamente en la clase envoltoria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0021c1-61b4-43b3-af83-96a4a2fdb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __getattr__(self, name):\n",
    "        # Check if the attribute exists in the underlying dataset\n",
    "        if hasattr(self.dataset, name):\n",
    "            return getattr(self.dataset, name)\n",
    "        raise AttributeError(\n",
    "            f\"'{type(self).__name__}' and its underlying dataset have no attribute '{name}'\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293890c-ebc6-470a-bcce-0b12ac0a438a",
   "metadata": {},
   "source": [
    "### Clase DataModule\r\n",
    "\r\n",
    "Esta clase es una subclase de `pytorch_lightning.LightningDataModule` y se utiliza para definir el módulo de datos en PyTorch Lightning. Aquí está una descripción de sus componentes:\r\n",
    "\r\n",
    "- `def __init__(self, hparams, dataset=None)`: El método de inicialización de la clase toma dos argumentos: `hparams`, que son los parámetros del hiperentrenamiento, y `dataset`, que es el conjunto de datos opcional que se puede proporcionar. \r\n",
    "\r\n",
    "- `super(DataModule, self).__init__()`: Llama al constructor de la clase base `LightningDataModule` para inicializar la clase.\r\n",
    "\r\n",
    "- `self.save_hyperparameters(hparams)`: Utiliza el método `save_hyperparameters` proporcionado por PyTorch Lightning para guardar los hiperparámetros en el objeto del módulo de datos. Esto es útil para registrar y rastrear automáticamente los hiperparámetros durante el entrenamiento.\r\n",
    "\r\n",
    "- `self._mean, self._std = None, None`: Inicializa las variables `_mean` y `_std` como `None`. Estas variables se utilizan para almacenar las medias y desviaciones estándar de los datos, si es necesario.\r\n",
    "\r\n",
    "- `self._saved_dataloaders = dict()`: Inicializa un diccionario vacío `_saved_dataloaders` para almacenar los dataloaders generados durante el proceso de entrenamiento.\r\n",
    "\r\n",
    "- `self.dataset = dataset`: Almacena el conjunto de datos proporcionado como atributo de la clase.\r\n",
    "\r\n",
    "Esta clase proporciona una base para definir y configurar el módulo de datos en un proyecto de PyTorch Lightning. Al extender esta clase, se pueden implementar los métodos necesarios para preparar y cargar los datos, así como para configurar los dataloaders y realizar cualquier preprocesamiento adicional necesario.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880cb00-8e50-464c-9158-e0264036e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, hparams, dataset=None):\n",
    "        super(DataModule, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self._mean, self._std = None, None\n",
    "        self._saved_dataloaders = dict()\n",
    "        self.dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae2892-a8c7-4c5c-a29d-463d4f1c27c9",
   "metadata": {},
   "source": [
    "### Método setup\r\n",
    "\r\n",
    "Este método se utiliza para preparar los datos para el entrenamiento, incluyendo la carga del conjunto de datos, la creación de divisiones de entrenamiento, validación y prueba, y cualquier preprocesamiento adicional necesario. Aquí está una descripción de su funcionalidad:\r\n",
    "\r\n",
    "- `def setup(self, stage)`: Define un método llamado `setup` que toma `stage` como argumento, indicando la etapa del proceso de entrenamiento.\r\n",
    "\r\n",
    "- `if self.dataset is None:`: Comprueba si el conjunto de datos ya está definido. Si no lo está, procede a cargar el conjunto de datos según la configuración especificada en los hiperparámetros.\r\n",
    "\r\n",
    "- `if self.hparams[\"dataset\"] == \"Custom\":`: Si se especifica un conjunto de datos personalizado en los hiperparámetros, se crea una instancia del conjunto de datos personalizado (`datasets.Custom`) utilizando los archivos de coordenadas, incrustaciones, energías y fuerzas proporcionados en los hiperparámetros.\r\n",
    "\r\n",
    "- `else:`: Si se utiliza un conjunto de datos predefinido, se crea una instancia del conjunto de datos correspondiente utilizando la configuración de ruta y cualquier argumento adicional proporcionado en los hiperparámetros.\r\n",
    "\r\n",
    "- `self.dataset = FloatCastDatasetWrapper(...)`: Una vez que se carga el conjunto de datos, se envuelve en un `FloatCastDatasetWrapper` para asegurarse de que todos los tensores sean del tipo de dato flotante especificado en los hiperparámetros.\r\n",
    "\r\n",
    "- `self.idx_train, self.idx_val, self.idx_test = make_splits(...)`: Se generan divisiones de entrenamiento, validación y prueba utilizando la función `make_splits`, que toma en cuenta el tamaño de cada conjunto, la semilla aleatoria y el directorio de registros para guardar las divisiones.\r\n",
    "\r\n",
    "- `self.train_dataset = Subset(self.dataset, self.idx_train)`: Se crea un subconjunto de entrenamiento a partir del conjunto de datos completo utilizando los índices de entrenamiento generados previamente.\r\n",
    "\r\n",
    "- `self.val_dataset = Subset(self.dataset, self.idx_val)`: Se crea un subconjunto de validación a partir del conjunto de datos completo utilizando los índices de validación generados previamente.\r\n",
    "\r\n",
    "- `self.test_dataset = Subset(self.dataset, self.idx_test)`: Se crea un subconjunto de prueba a partir del conjunto de datos completo utilizando los índices de prueba generados previamente.\r\n",
    "\r\n",
    "- `if self.hparams[\"standardize\"]:`: Si se especifica en los hiperparámetros, se realiza una estandarización de los datos llamando al método `_standardize`.\r\n",
    "\r\n",
    "Este método garantiza que los datos estén preparados y divididos adecuadamente para el entrenamiento del modelo, lo que facilita la configuración y ejecución del proceso de entrenamiento en PyTorch Lightning.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c632c1e7-c262-47c2-907f-66ec88bc6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def setup(self, stage):\n",
    "        if self.dataset is None:\n",
    "            if self.hparams[\"dataset\"] == \"Custom\":\n",
    "                self.dataset = datasets.Custom(\n",
    "                    self.hparams[\"coord_files\"],\n",
    "                    self.hparams[\"embed_files\"],\n",
    "                    self.hparams[\"energy_files\"],\n",
    "                    self.hparams[\"force_files\"],\n",
    "                )\n",
    "            else:\n",
    "                dataset_arg = {}\n",
    "                if self.hparams[\"dataset_arg\"] is not None:\n",
    "                    dataset_arg = self.hparams[\"dataset_arg\"]\n",
    "                self.dataset = getattr(datasets, self.hparams[\"dataset\"])(\n",
    "                    self.hparams[\"dataset_root\"], **dataset_arg\n",
    "                )\n",
    "        self.dataset = FloatCastDatasetWrapper(\n",
    "            self.dataset, dtype_mapping[self.hparams[\"precision\"]]\n",
    "        )\n",
    "\n",
    "        self.idx_train, self.idx_val, self.idx_test = make_splits(\n",
    "            len(self.dataset),\n",
    "            self.hparams[\"train_size\"],\n",
    "            self.hparams[\"val_size\"],\n",
    "            self.hparams[\"test_size\"],\n",
    "            self.hparams[\"seed\"],\n",
    "            join(self.hparams[\"log_dir\"], \"splits.npz\"),\n",
    "            self.hparams[\"splits\"],\n",
    "        )\n",
    "        print(\n",
    "            f\"train {len(self.idx_train)}, val {len(self.idx_val)}, test {len(self.idx_test)}\"\n",
    "        )\n",
    "\n",
    "        self.train_dataset = Subset(self.dataset, self.idx_train)\n",
    "        self.val_dataset = Subset(self.dataset, self.idx_val)\n",
    "        self.test_dataset = Subset(self.dataset, self.idx_test)\n",
    "\n",
    "        if self.hparams[\"standardize\"]:\n",
    "            self._standardize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15602419-d54e-4731-8a98-6572e0800a8a",
   "metadata": {},
   "source": [
    "### Método train_dataloader\r\n",
    "\r\n",
    "Este método se utiliza para crear y devolver un DataLoader para el conjunto de datos de entrenamiento. Aquí está una descripción de su funcionalidad:\r\n",
    "\r\n",
    "- `def train_dataloader(self)`: Define un método llamado `train_dataloader` que no toma ningún argumento.\r\n",
    "\r\n",
    "- `return self._get_dataloader(self.train_dataset, \"train\")`: Devuelve un DataLoader para el conjunto de datos de entrenamiento llamando al método `_get_dataloader`. El método `_get_dataloader` se encarga de crear el DataLoader con el conjunto de datos especificado y un indicador para identificar el tipo de DataLoader, que en este caso es \"train\".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36641a1f-920c-4630-a77d-add381069969",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train_dataloader(self):\n",
    "        return self._get_dataloader(self.train_dataset, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705877a-b373-4a71-8027-9cc268cf173c",
   "metadata": {},
   "source": [
    "### Método val_dataloader\r\n",
    "\r\n",
    "Este método se utiliza para crear y devolver los DataLoaders para el conjunto de datos de validación y, opcionalmente, para el conjunto de datos de prueba. Aquí está una descripción de su funcionalidad:\r\n",
    "\r\n",
    "- `def val_dataloader(self)`: Define un método llamado `val_dataloader` que no toma ningún argumento.\r\n",
    "\r\n",
    "- `loaders = [self._get_dataloader(self.val_dataset, \"val\")]`: Crea un DataLoader para el conjunto de datos de validación llamando al método `_get_dataloader`. Este DataLoader se agrega a una lista llamada `loaders`.\r\n",
    "\r\n",
    "- `if (...)`: Comprueba si hay un conjunto de datos de prueba disponible y si es el momento de incluirlo en la validación, según el intervalo de prueba especificado en los hiperparámetros y el número actual de épocas.\r\n",
    "\r\n",
    "- `loaders.append(self._get_dataloader(self.test_dataset, \"test\"))`: Si se cumple la condición anterior, se crea un DataLoader para el conjunto de datos de prueba y se agrega a la lista `loaders`.\r\n",
    "\r\n",
    "- `return loaders`: Devuelve la lista de DataLoaders, que contiene los DataLoaders para los conjuntos de datos de validación y, opcionalmente, de prueba.\r\n",
    "\r\n",
    "Este método proporciona una forma flexible de obtener los DataLoaders para los conjuntos de datos de validación y prueba, lo que permite realizar evaluaciones durante el entrenamiento y ajustar el proceso de entrenamiento según sea necesario.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f26ce0-53ad-4bac-9cde-e61602631230",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def val_dataloader(self):\n",
    "        loaders = [self._get_dataloader(self.val_dataset, \"val\")]\n",
    "        if (\n",
    "            len(self.test_dataset) > 0\n",
    "            and (self.trainer.current_epoch + 1) % self.hparams[\"test_interval\"] == 0\n",
    "        ):\n",
    "            loaders.append(self._get_dataloader(self.test_dataset, \"test\"))\n",
    "        return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0154de97-d43b-4093-a54e-637660fc6ad3",
   "metadata": {},
   "source": [
    "### Método test_dataloader\r\n",
    "\r\n",
    "Este método se utiliza para crear y devolver un DataLoader para el conjunto de datos de prueba. Aquí está una descripción de su funcionalidad:\r\n",
    "\r\n",
    "- `def test_dataloader(self)`: Define un método llamado `test_dataloader` que no toma ningún argumento.\r\n",
    "\r\n",
    "- `return self._get_dataloader(self.test_dataset, \"test\")`: Devuelve un DataLoader para el conjunto de datos de prueba llamando al método `_get_dataloader`. El método `_get_dataloader` se encarga de crear el DataLoader con el conjunto de datos especificado y un indicador para identificar el tipo de DataLoader, que en este caso es \"test\".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d7128-70d7-41de-aaa8-e41656feab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def test_dataloader(self):\n",
    "        return self._get_dataloader(self.test_dataset, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30686105-8aaf-4cc3-8cf3-95f943949f78",
   "metadata": {},
   "source": [
    "### Método atomref (propiedad)\r\n",
    "\r\n",
    "Este método es una propiedad que se utiliza para obtener la referencia a los átomos del conjunto de datos. Aquí está una descripción de su funcionalidad:\r\n",
    "\r\n",
    "- `@property`: Este decorador indica que `atomref` es una propiedad, lo que permite acceder a ella como un atributo sin necesidad de llamar a un método.\r\n",
    "\r\n",
    "- `def atomref(self)`: Define un método llamado `atomref` que toma `self` como argumento.\r\n",
    "\r\n",
    "- `if hasattr(self.dataset, \"get_atomref\"):`: Comprueba si el conjunto de datos tiene un método llamado `get_atomref`. Si el método existe, se llama a `self.dataset.get_atomref()` para obtener la referencia a los átomos.\r\n",
    "\r\n",
    "- `return self.dataset.get_atomref()`: Devuelve la referencia a los átomos del conjunto de datos si el método `get_atomref` está disponible en el conjunto de datos.\r\n",
    "\r\n",
    "- `return None`: Si el conjunto de datos no tiene un método `get_atomref`, devuelve `None`.\r\n",
    "\r\n",
    "Este método proporciona una forma de obtener la referencia a los átomos del conjunto de datos, lo que puede ser útil para realizar ciertas operaciones o análisis adicionales en el conjunto de datos.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bcd11-8449-40f3-9715-5cdf81ba3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @property\n",
    "    def atomref(self):\n",
    "        if hasattr(self.dataset, \"get_atomref\"):\n",
    "            return self.dataset.get_atomref()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206454d-c98a-4b44-89f6-eb79fe72c497",
   "metadata": {},
   "source": [
    "### Propiedad mean\r\n",
    "\r\n",
    "Esta propiedad se utiliza para acceder a la media de los datos del conjunto de datos. Aquí está una descripción de su funcionalidad:\r\n",
    "\r\n",
    "- `@property`: Este decorador indica que `mean` es una propiedad, lo que permite acceder a ella como un atributo sin necesidad de llamar a un método.\r\n",
    "\r\n",
    "- `def mean(self)`: Define un método llamado `mean` que toma `self` como argumento.\r\n",
    "\r\n",
    "- `return self._mean`: Devuelve el valor almacenado en la variable `_mean`, que representa la media de los datos del conjunto de datos.\r\n",
    "\r\n",
    "Esta propiedad proporciona una forma de acceder a la media de los datos del conjunto de datos desde fuera de la clase, lo que puede ser útil para realizar análisis o visualizaciones adicionales.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa4ceb-b38a-434a-92f6-aefa347e3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @property\n",
    "    def mean(self):\n",
    "        return self._mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db9c6c-1c0e-4f8c-9add-c5f6845e939d",
   "metadata": {},
   "source": [
    "### Propiedad std\r\n",
    "\r\n",
    "Esta propiedad se utiliza para acceder a la desviación estándar de los datos del conjunto de datos. Aquí está una descripción de su funcionalidad:\r\n",
    "\r\n",
    "- `@property`: Este decorador indica que `std` es una propiedad, lo que permite acceder a ella como un atributo sin necesidad de llamar a un método.\r\n",
    "\r\n",
    "- `def std(self)`: Define un método llamado `std` que toma `self` como argumento.\r\n",
    "\r\n",
    "- `return self._std`: Devuelve el valor almacenado en la variable `_std`, que representa la desviación estándar de los datos del conjunto de datos.\r\n",
    "\r\n",
    "Esta propiedad proporciona una forma de acceder a la desviación estándar de los datos del conjunto de datos desde fuera de la clase, lo que puede ser útil para realizar análisis o visualizaciones adicionales.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f91f67-333c-4c44-b5b7-2cc116268dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @property\n",
    "    def std(self):\n",
    "        return self._std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820004e-1cc1-4828-b414-d674653e298e",
   "metadata": {},
   "source": [
    "### Método _get_dataloader\r\n",
    "\r\n",
    "Este método se utiliza para obtener un DataLoader para un conjunto de datos específico en una determinada etapa del proceso, como entrenamiento, validación o prueba. Aquí está una descripción de su funcionalidad:\r\n",
    "\r\n",
    "- `def _get_dataloader(self, dataset, stage, store_dataloader=True)`: Define un método privado llamado `_get_dataloader` que toma tres argumentos: `dataset`, el conjunto de datos para el que se desea crear el DataLoader; `stage`, la etapa del proceso para la que se necesita el DataLoader (por ejemplo, \"train\", \"val\" o \"test\"); y `store_dataloader`, un indicador booleano que determina si se debe almacenar el DataLoader para su posterior uso.\r\n",
    "\r\n",
    "- `store_dataloader = (...)` y `if stage in self._saved_dataloaders and store_dataloader`: Estos bloques de código gestionan si se debe almacenar el DataLoader para su posterior uso. Si `store_dataloader` es `True` y ya existe un DataLoader almacenado para la etapa específica, se devuelve el DataLoader almacenado en lugar de crear uno nuevo.\r\n",
    "\r\n",
    "- En la sección siguiente, se configura el DataLoader según la etapa especificada:\r\n",
    "    - Para la etapa de entrenamiento (`stage == \"train\"`), se utiliza el tamaño de lote y la opción de barajado especificados en los hiperparámetros.\r\n",
    "    - Para las etapas de validación y prueba (`stage in [\"val\", \"test\"]`), se utiliza el tamaño de lote de inferencia especificado en los hiperparámetros y la opción de barajado se establece en `False`.\r\n",
    "\r\n",
    "- `dl = DataLoader(...)`: Se crea el DataLoader con los parámetros especificados para el conjunto de datos.\r\n",
    "\r\n",
    "- `if store_dataloader:` y `self._saved_dataloaders[stage] = dl`: Si `store_dataloader` es `True`, se almacena el DataLoader en el diccionario `_saved_dataloaders` para su posterior uso.\r\n",
    "\r\n",
    "- `return dl`: Se devuelve el DataLoader creado o almacenado.\r\n",
    "\r\n",
    "Este método proporciona una forma de obtener y gestionar DataLoaders de manera eficiente para diferentes etapas del proceso, garantizando que los DataLoaders se creen solo cuando sea necesario y se puedan almacenar para su posterior reutilización si es necesario.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac8f6f-f5af-463a-99bd-55fbdb746f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_dataloader(self, dataset, stage, store_dataloader=True):\n",
    "        store_dataloader = (\n",
    "            store_dataloader and self.trainer.reload_dataloaders_every_n_epochs <= 0\n",
    "        )\n",
    "        if stage in self._saved_dataloaders and store_dataloader:\n",
    "            # storing the dataloaders like this breaks calls to trainer.reload_train_val_dataloaders\n",
    "            # but makes it possible that the dataloaders are not recreated on every testing epoch\n",
    "            return self._saved_dataloaders[stage]\n",
    "\n",
    "        if stage == \"train\":\n",
    "            batch_size = self.hparams[\"batch_size\"]\n",
    "            shuffle = True\n",
    "        elif stage in [\"val\", \"test\"]:\n",
    "            batch_size = self.hparams[\"inference_batch_size\"]\n",
    "            shuffle = False\n",
    "\n",
    "        dl = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "            pin_memory=True,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "\n",
    "        if store_dataloader:\n",
    "            self._saved_dataloaders[stage] = dl\n",
    "        return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94d391-b669-4db9-880c-427b8137b309",
   "metadata": {},
   "source": [
    "### Método _standardize\r\n",
    "\r\n",
    "Este método se utiliza para estandarizar los datos del conjunto de entrenamiento, calculando la media y la desviación estándar de las energías en el conjunto de datos de entrenamiento. Aquí está una descripción de su funcionalidad:\r\n",
    "\r\n",
    "- `def _standardize(self)`: Define un método privado llamado `_standardize` que no toma ningún argumento.\r\n",
    "\r\n",
    "- `def get_energy(batch, atomref)`: Define una función interna llamada `get_energy` que toma un lote de datos y una referencia de átomos como argumentos y devuelve las energías del lote.\r\n",
    "\r\n",
    "- `data = tqdm(...)`: Utiliza la función `tqdm` para iterar sobre un DataLoader que contiene el conjunto de datos de entrenamiento y calcular la media y la desviación estándar de las energías en el conjunto de datos.\r\n",
    "\r\n",
    "- `atomref = ...`: Determina si se está utilizando un modelo de referencia de átomos. Si es así, se utiliza la referencia de átomos del módulo de datos (`self.atomref`), de lo contrario, se establece en `None`.\r\n",
    "\r\n",
    "- `ys = torch.cat(...)`: Extrae las energías de los lotes de datos y las concatena en un tensor `ys`.\r\n",
    "\r\n",
    "- `self._mean = ys.mean(dim=0)`: Calcula la media de las energías a lo largo de la dimensión 0 y la almacena en el atributo `_mean`.\r\n",
    "\r\n",
    "- `self._std = ys.std(dim=0)`: Calcula la desviación estándar de las energías a lo largo de la dimensión 0 y la almacena en el atributo `_std`.\r\n",
    "\r\n",
    "Este método es importante para normalizar los datos de entrada durante el entrenamiento del modelo, lo que puede ayudar a mejorar la convergencia y el rendimiento del modelo. La media y la desviación estándar calculadas se utilizan luego para normalizar los datos de entrada durante la inferencia.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2070a-d88f-4721-a0a0-6e4a59f30bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _standardize(self):\n",
    "        def get_energy(batch, atomref):\n",
    "            if \"y\" not in batch or batch.y is None:\n",
    "                raise MissingEnergyException()\n",
    "\n",
    "            if atomref is None:\n",
    "                return batch.y.clone()\n",
    "\n",
    "            # remove atomref energies from the target energy\n",
    "            atomref_energy = scatter(atomref[batch.z], batch.batch, dim=0)\n",
    "            return (batch.y.squeeze() - atomref_energy.squeeze()).clone()\n",
    "\n",
    "        data = tqdm(\n",
    "            self._get_dataloader(self.train_dataset, \"val\", store_dataloader=False),\n",
    "            desc=\"computing mean and std\",\n",
    "        )\n",
    "        try:\n",
    "            # only remove atomref energies if the atomref prior is used\n",
    "            atomref = self.atomref if self.hparams[\"prior_model\"] == \"Atomref\" else None\n",
    "            # extract energies from the data\n",
    "            ys = torch.cat([get_energy(batch, atomref) for batch in data])\n",
    "        except MissingEnergyException:\n",
    "            rank_zero_warn(\n",
    "                \"Standardize is true but failed to compute dataset mean and \"\n",
    "                \"standard deviation. Maybe the dataset only contains forces.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # compute mean and standard deviation\n",
    "        self._mean = ys.mean(dim=0)\n",
    "        self._std = ys.std(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6a029-298a-4fe3-8806-efb227ba2d98",
   "metadata": {},
   "source": [
    "## scripts/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d4839-4afc-4e9f-a66e-d2d304fda7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger, WandbLogger\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "from torchmdnet.module import LNNP\n",
    "from torchmdnet import datasets, priors, models\n",
    "from torchmdnet.data import DataModule\n",
    "from torchmdnet.models import output_modules\n",
    "from torchmdnet.models.model import create_prior_models\n",
    "from torchmdnet.models.utils import rbf_class_mapping, act_class_mapping, dtype_mapping\n",
    "from torchmdnet.utils import LoadFromFile, LoadFromCheckpoint, save_argparse, number\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88f847-323a-4e9e-bf3a-6f8b7b4ede34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # fmt: off\n",
    "    parser = argparse.ArgumentParser(description='Training')\n",
    "    parser.add_argument('--load-model', action=LoadFromCheckpoint, help='Restart training using a model checkpoint')  # keep first\n",
    "    parser.add_argument('--conf', '-c', type=open, action=LoadFromFile, help='Configuration yaml file')  # keep second\n",
    "    parser.add_argument('--num-epochs', default=300, type=int, help='number of epochs')\n",
    "    parser.add_argument('--batch-size', default=32, type=int, help='batch size')\n",
    "    parser.add_argument('--inference-batch-size', default=None, type=int, help='Batchsize for validation and tests.')\n",
    "    parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--lr-patience', type=int, default=10, help='Patience for lr-schedule. Patience per eval-interval of validation')\n",
    "    parser.add_argument('--lr-metric', type=str, default='val_loss', choices=['train_loss', 'val_loss'], help='Metric to monitor when deciding whether to reduce learning rate')\n",
    "    parser.add_argument('--lr-min', type=float, default=1e-6, help='Minimum learning rate before early stop')\n",
    "    parser.add_argument('--lr-factor', type=float, default=0.8, help='Factor by which to multiply the learning rate when the metric stops improving')\n",
    "    parser.add_argument('--lr-warmup-steps', type=int, default=0, help='How many steps to warm-up over. Defaults to 0 for no warm-up')\n",
    "    parser.add_argument('--early-stopping-patience', type=int, default=30, help='Stop training after this many epochs without improvement')\n",
    "    parser.add_argument('--reset-trainer', type=bool, default=False, help='Reset training metrics (e.g. early stopping, lr) when loading a model checkpoint')\n",
    "    parser.add_argument('--weight-decay', type=float, default=0.0, help='Weight decay strength')\n",
    "    parser.add_argument('--ema-alpha-y', type=float, default=1.0, help='The amount of influence of new losses on the exponential moving average of y')\n",
    "    parser.add_argument('--ema-alpha-neg-dy', type=float, default=1.0, help='The amount of influence of new losses on the exponential moving average of dy')\n",
    "    parser.add_argument('--ngpus', type=int, default=-1, help='Number of GPUs, -1 use all available. Use CUDA_VISIBLE_DEVICES=1, to decide gpus')\n",
    "    parser.add_argument('--num-nodes', type=int, default=1, help='Number of nodes')\n",
    "    parser.add_argument('--precision', type=int, default=32, choices=[16, 32, 64], help='Floating point precision')\n",
    "    parser.add_argument('--log-dir', '-l', default='/tmp/logs', help='log file')\n",
    "    parser.add_argument('--splits', default=None, help='Npz with splits idx_train, idx_val, idx_test')\n",
    "    parser.add_argument('--train-size', type=number, default=None, help='Percentage/number of samples in training set (None to use all remaining samples)')\n",
    "    parser.add_argument('--val-size', type=number, default=0.05, help='Percentage/number of samples in validation set (None to use all remaining samples)')\n",
    "    parser.add_argument('--test-size', type=number, default=0.1, help='Percentage/number of samples in test set (None to use all remaining samples)')\n",
    "    parser.add_argument('--test-interval', type=int, default=10, help='Test interval, one test per n epochs (default: 10)')\n",
    "    parser.add_argument('--save-interval', type=int, default=10, help='Save interval, one save per n epochs (default: 10)')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n",
    "    parser.add_argument('--num-workers', type=int, default=4, help='Number of workers for data prefetch')\n",
    "    parser.add_argument('--redirect', type=bool, default=False, help='Redirect stdout and stderr to log_dir/log')\n",
    "    parser.add_argument('--gradient-clipping', type=float, default=0.0, help='Gradient clipping norm')\n",
    "\n",
    "    # dataset specific\n",
    "    parser.add_argument('--dataset', default=None, type=str, choices=datasets.__all__, help='Name of the torch_geometric dataset')\n",
    "    parser.add_argument('--dataset-root', default='~/data', type=str, help='Data storage directory (not used if dataset is \"CG\")')\n",
    "    parser.add_argument('--dataset-arg', default=None, type=str, help='Additional dataset arguments, e.g. target property for QM9 or molecule for MD17. Need to be specified in JSON format i.e. \\'{\"molecules\": \"aspirin,benzene\"}\\'')\n",
    "    parser.add_argument('--coord-files', default=None, type=str, help='Custom coordinate files glob')\n",
    "    parser.add_argument('--embed-files', default=None, type=str, help='Custom embedding files glob')\n",
    "    parser.add_argument('--energy-files', default=None, type=str, help='Custom energy files glob')\n",
    "    parser.add_argument('--force-files', default=None, type=str, help='Custom force files glob')\n",
    "    parser.add_argument('--y-weight', default=1.0, type=float, help='Weighting factor for y label in the loss function')\n",
    "    parser.add_argument('--neg-dy-weight', default=1.0, type=float, help='Weighting factor for neg_dy label in the loss function')\n",
    "\n",
    "    # model architecture\n",
    "    parser.add_argument('--model', type=str, default='graph-network', choices=models.__all__, help='Which model to train')\n",
    "    parser.add_argument('--output-model', type=str, default='Scalar', choices=output_modules.__all__, help='The type of output model')\n",
    "    parser.add_argument('--prior-model', type=str, default=None, choices=priors.__all__, help='Which prior model to use')\n",
    "\n",
    "    # architectural args\n",
    "    parser.add_argument('--charge', type=bool, default=False, help='Model needs a total charge')\n",
    "    parser.add_argument('--spin', type=bool, default=False, help='Model needs a spin state')\n",
    "    parser.add_argument('--embedding-dimension', type=int, default=256, help='Embedding dimension')\n",
    "    parser.add_argument('--num-layers', type=int, default=6, help='Number of interaction layers in the model')\n",
    "    parser.add_argument('--num-rbf', type=int, default=64, help='Number of radial basis functions in model')\n",
    "    parser.add_argument('--activation', type=str, default='silu', choices=list(act_class_mapping.keys()), help='Activation function')\n",
    "    parser.add_argument('--rbf-type', type=str, default='expnorm', choices=list(rbf_class_mapping.keys()), help='Type of distance expansion')\n",
    "    parser.add_argument('--trainable-rbf', type=bool, default=False, help='If distance expansion functions should be trainable')\n",
    "    parser.add_argument('--neighbor-embedding', type=bool, default=False, help='If a neighbor embedding should be applied before interactions')\n",
    "    parser.add_argument('--aggr', type=str, default='add', help='Aggregation operation for CFConv filter output. Must be one of \\'add\\', \\'mean\\', or \\'max\\'')\n",
    "\n",
    "    # Transformer specific\n",
    "    parser.add_argument('--distance-influence', type=str, default='both', choices=['keys', 'values', 'both', 'none'], help='Where distance information is included inside the attention')\n",
    "    parser.add_argument('--attn-activation', default='silu', choices=list(act_class_mapping.keys()), help='Attention activation function')\n",
    "    parser.add_argument('--num-heads', type=int, default=8, help='Number of attention heads')\n",
    "    \n",
    "    # TensorNet specific\n",
    "    parser.add_argument('--equivariance-invariance-group', type=str, default='O(3)', help='Equivariance and invariance group of TensorNet')\n",
    "\n",
    "    # other args\n",
    "    parser.add_argument('--derivative', default=False, type=bool, help='If true, take the derivative of the prediction w.r.t coordinates')\n",
    "    parser.add_argument('--cutoff-lower', type=float, default=0.0, help='Lower cutoff in model')\n",
    "    parser.add_argument('--cutoff-upper', type=float, default=5.0, help='Upper cutoff in model')\n",
    "    parser.add_argument('--atom-filter', type=int, default=-1, help='Only sum over atoms with Z > atom_filter')\n",
    "    parser.add_argument('--max-z', type=int, default=100, help='Maximum atomic number that fits in the embedding matrix')\n",
    "    parser.add_argument('--max-num-neighbors', type=int, default=32, help='Maximum number of neighbors to consider in the network')\n",
    "    parser.add_argument('--standardize', type=bool, default=False, help='If true, multiply prediction by dataset std and add mean')\n",
    "    parser.add_argument('--reduce-op', type=str, default='add', choices=['add', 'mean'], help='Reduce operation to apply to atomic predictions')\n",
    "    parser.add_argument('--wandb-use', default=False, type=bool, help='Defines if wandb is used or not')\n",
    "    parser.add_argument('--wandb-name', default='training', type=str, help='Give a name to your wandb run')\n",
    "    parser.add_argument('--wandb-project', default='training_', type=str, help='Define what wandb Project to log to')\n",
    "    parser.add_argument('--wandb-resume-from-id', default=None, type=str, help='Resume a wandb run from a given run id. The id can be retrieved from the wandb dashboard')\n",
    "    parser.add_argument('--tensorboard-use', default=False, type=bool, help='Defines if tensor board is used or not')\n",
    "\n",
    "    # fmt: on\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.redirect:\n",
    "        sys.stdout = open(os.path.join(args.log_dir, \"log\"), \"w\")\n",
    "        sys.stderr = sys.stdout\n",
    "        logging.getLogger(\"pytorch_lightning\").addHandler(\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        )\n",
    "\n",
    "    if args.inference_batch_size is None:\n",
    "        args.inference_batch_size = args.batch_size\n",
    "\n",
    "    save_argparse(args, os.path.join(args.log_dir, \"input.yaml\"), exclude=[\"conf\"])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bc732-6fa6-46e6-9555-639453db99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = get_args()\n",
    "    pl.seed_everything(args.seed, workers=True)\n",
    "\n",
    "    # initialize data module\n",
    "    data = DataModule(args)\n",
    "    data.prepare_data()\n",
    "    data.setup(\"fit\")\n",
    "\n",
    "    prior_models = create_prior_models(vars(args), data.dataset)\n",
    "    args.prior_args = [p.get_init_args() for p in prior_models]\n",
    "\n",
    "    # initialize lightning module\n",
    "    model = LNNP(args, prior_model=prior_models, mean=data.mean, std=data.std)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=args.log_dir,\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=10,  # -1 to save all\n",
    "        every_n_epochs=args.save_interval,\n",
    "        filename=\"{epoch}-{val_loss:.4f}-{test_loss:.4f}\",\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\"val_loss\", patience=args.early_stopping_patience)\n",
    "\n",
    "    csv_logger = CSVLogger(args.log_dir, name=\"\", version=\"\")\n",
    "    _logger = [csv_logger]\n",
    "    if args.wandb_use:\n",
    "        wandb_logger = WandbLogger(\n",
    "            project=args.wandb_project,\n",
    "            name=args.wandb_name,\n",
    "            save_dir=args.log_dir,\n",
    "            resume=\"must\" if args.wandb_resume_from_id is not None else None,\n",
    "            id=args.wandb_resume_from_id,\n",
    "        )\n",
    "        _logger.append(wandb_logger)\n",
    "\n",
    "    if args.tensorboard_use:\n",
    "        tb_logger = pl.loggers.TensorBoardLogger(\n",
    "            args.log_dir, name=\"tensorbord\", version=\"\", default_hp_metric=False\n",
    "        )\n",
    "        _logger.append(tb_logger)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        strategy=DDPStrategy(find_unused_parameters=False),\n",
    "        max_epochs=args.num_epochs,\n",
    "        gpus=args.ngpus,\n",
    "        num_nodes=args.num_nodes,\n",
    "        default_root_dir=args.log_dir,\n",
    "        auto_lr_find=False,\n",
    "        resume_from_checkpoint=None if args.reset_trainer else args.load_model,\n",
    "        callbacks=[early_stopping, checkpoint_callback],\n",
    "        logger=_logger,\n",
    "        precision=args.precision,\n",
    "        gradient_clip_val=args.gradient_clipping,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, data)\n",
    "\n",
    "    # run test set after completing the fit\n",
    "    model = LNNP.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    trainer = pl.Trainer(logger=_logger)\n",
    "    trainer.test(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c178f54e-1fa1-4bd3-8b00-90f9cea33da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
